---
title: SparkåŸºç¡€ç¯å¢ƒé…ç½®
date: 2022-05-22 02:50:33
tags: SparkåŸºç¡€ç¯å¢ƒé…ç½®
---



##### *åšä¸»çš„æœ¬åœ°ç¯å¢ƒä¸ºï¼šï£¿Macbok pro , macOS Monterey 12.3.1*

* Mac ç”¨æˆ·æ³¨æ„éœ€è¦æ³¨æ„ï¼Œè™šæ‹Ÿæœºä½¿ç”¨VMware Fusionï¼Œé•œåƒä¸ºCentOS7.

-----



## ä¸€ã€åŸºç¡€é…ç½®

#### **1.é…ç½®ç½‘ç»œé€‚é…**

Macç”¨æˆ·åªéœ€ç‚¹å‡»VMware Fusion > åå¥½è®¾ç½® > ç½‘ç»œï¼Œå¹¶ç‚¹å‡»å·¦ä¸‹è§’â€œ + â€å³å¯æ·»åŠ è™šæ‹Ÿæœºä¸Šæ‰€ç”¨çš„å­ç½‘ipï¼ˆä¾‹ï¼š192.168.138.0ï¼‰ã€‚

![1.png](./SparkåŸºç¡€ç¯å¢ƒé…ç½®/1.png)

*æ³¨ï¼šMacç”¨æˆ·è‹¥çœç•¥æ­¤å¤„æ­¥éª¤å¯èƒ½ä¼šå¯¼è‡´æ—¶é—´çš„æ¨ç§»è€Œå¯¼è‡´pingä¸åŠ¨ç½‘ç»œï¼ˆè¡€çš„æ•™è®­ğŸ˜­ï¼‰*

* åˆ†åˆ«ä¿®æ”¹ä¸‰å°è™šæ‹Ÿæœºçš„ä¸»æœºå

  ```shell
  # node1
  echo "node1" >/etc/hostname
  # node2
  echo "node2" >/etc/hostname
  # node3
  echo "node3" >/etc/hostname
  ```

* æ›´è¯¥ç½‘ç»œé…ç½® ï¼ˆnode1 node2 node3ï¼‰

  ```shell
  vim /etc/sysconfig/network-scripts/ifcfg-ens33(node2,node3ä¸­çš„IPADDRåªéœ€åœ¨151ä¸Šé€’åŠ å³å¯)
  
  TYPE="Ethernet"
  PROXY_METHOD="none"
  BROWSER_ONLY="no"
  BOOTPROTO="static"# é…ç½®BOOTPROTO=staticï¼šè¡¨ç¤ºé™æ€è·¯ç”±åè®®ï¼Œå¯ä»¥ä¿æŒIPå›ºå®š
  DEFROUTE="yes"
  IPV4_FAILURE_FATAL="no"
  IPV6INIT="yes"
  IPV6_AUTOCONF="yes"
  IPV6_DEFROUTE="yes"
  IPV6_FAILURE_FATAL="no"
  IPV6_ADDR_GEN_MODE="stable-privacy"
  NAME="ens33"
  UUID="7815751b-505d-4ae2-b2d4-aa39591dc6aa"
  DEVICE="ens33"
  ONBOOT="yes"# é…ç½®ONBOOT=yes:è¡¨ç¤ºå¯åŠ¨è¿™å—ç½‘å¡
  IPADDR="192.168.138.151"# é…ç½®IPADDR:è¡¨ç¤ºè™šæ‹Ÿæœºçš„IPåœ°å€ï¼Œè¿™é‡Œè®¾ç½®çš„IPåœ°å€è¦ä¸å‰é¢IPæ˜ å°„é…ç½®æ—¶çš„IPåœ°å€ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•é€šè¿‡ä¸»æœºåæ‰¾åˆ°å¯¹åº”IP;
  PREFIX="24"
  GATEWAY="192.168.138.2"# é…ç½®GATEWAY:è¡¨ç¤ºè™šæ‹Ÿæœºç½‘å…³,é€šå¸¸éƒ½æ˜¯å°†IPåœ°å€æœ€åä¸€ä¸ªä½æ•°å˜ä¸º2;
  DNS1="192.168.138.2"# é…ç½®DNS1:è¡¨ç¤ºåŸŸåè§£æå™¨ï¼Œæ­¤å¤„é‡‡ç”¨Googleæä¾›çš„å…è´¹DNSæœåŠ¡å™¨88.8.8(ä¹Ÿå¯ä»¥è®¾ç½®ä¸ºPCç«¯ç”µè„‘å¯¹åº”çš„DNS)ã€‚
  DOMAIN="114.114.114.114"
  IPV6_PRIVACY="no"
  ```

* æ›´æ”¹å®Œæˆåå¯¹ä¸‰å°è™šæ‹Ÿæœºè¿›è¡Œé‡å¯

  ```shell
  shutdown -r 0
  ```

* é‡å¯æˆåŠŸåè¿›è¡Œä¸€æ¬¡è¿é€šå¤–ç½‘çš„æµ‹è¯•

  ```shell
  ping baidu.com -c3
  ```

#### 2.ä¿®æ”¹hostæ˜ å°„

ä¿®æ”¹/etc/hostsæ˜ å°„æ–‡ä»¶ï¼Œä¸ºçš„æ˜¯å°†æ¥å¯¹ç›¸å…³è”çš„è™šæ‹Ÿæœºæ“ä½œæ›´åŠ ä¾¿æ·ï¼Œå…¶ä¸­æ·»åŠ ç›¸äº’å¯¹åº”çš„IPåœ°å€å’Œä¸»æœºåã€‚

```shell
vim /etc/hosts

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.138.151 node1 node1.itcast.cn
192.168.138.152 node2 node2.itcast.cn
192.168.138.153 node3 node3.itcast.cn                                 
```

æ–°æ‰‹é¡»çŸ¥ï¼šæ¨å‡ºå¹¶ä¿å­˜çš„æ–¹å¼å¯ä¸º â€˜esc + xâ€˜ï¼Œâ€™esc + shift+zzâ€˜

#### 3.é›†ç¾¤çš„æ—¶é—´åŒæ­¥

æˆ‘ä»¬æœ‰çš„æ—¶å€™åœ¨ä½¿ç”¨è™šæ‹Ÿæœºæ—¶é‡åˆ°æ•°æ®è¯¯å·®çš„æ—¶å€™ï¼Œæ€»æ˜¯ä¸æ¸…æ¥šå“ªé‡Œå‡ºé”™ï¼ŒåŸå› å°±åœ¨å¯èƒ½æ²¡æœ‰è¿›è¡Œæ—¶é—´åŒæ­¥ã€‚åœ¨æ•´ä¸ªé›†ç¾¤å½“ä¸­ï¼Œæ—¶é—´åŒæ­¥å……å½“ååˆ†é‡è¦çš„è§’è‰²ã€‚ä¸è¦å­˜åœ¨ä¾¥å¹¸å¿ƒç†ï¼Œå¦‚è‹¥å› ä¸ºæ—¶é—´ä¸åŒæ­¥é€ æˆäº†æŸå¤±ï¼Œå³ä½¿æ˜¯è¿›è¡Œäº†æ•°æ®å¤‡ä»½ï¼Œé‚£ä¹ˆä½ å¯èƒ½æ— æ³•åœ¨æ­£ç¡®çš„æ—¶é—´å°†æ­£ç¡®çš„æ•°æ®è¿›è¡Œå¤‡ä»½ã€‚

```shell
ntpdate ntp5.aliyun.com
```

å»ºè®®ï¼šåœ¨æ¯æ¬¡æ‰§è¡Œpoweroffä¹‹åçš„å†å¯åŠ¨åæ‰§è¡Œã€‚

#### 4. SSHå…å¯†é’¥ç™»é™†

* åœ¨node1ä¸Šç”Ÿæˆå…¬é’¥ç§é’¥

  ```shell
  ssh-keygen
  
  Generating public/private rsa key pair.
  Enter file in which to save the key (/root/.ssh/id_rsa): 
  Enter passphrase (empty for no passphrase): 
  Enter same passphrase again: 
  Your identification has been saved in /root/.ssh/id_rsa.
  Your public key has been saved in /root/.ssh/id_rsa.pub.
  The key fingerprint is:
  SHA256:QUAgFH5KBc/Erlf1JWSBbKeEepPJqMBqpWbc02/uFj8 root@master
  The key's randomart image is:
  +---[RSA 2048]----+
  | .=++oo+.o+.     |
  | . *. ..*.o .    |
  |. o.++ *.+ o     |
  |.o ++ B ...      |
  |o.=o.o .S        |
  |.*oo.. .         |
  |+  .. . o        |
  |       + E       |
  |      =o  .      |
  +----[SHA256]-----+
  ```

* åœ¨node1ä¸Šé…ç½®å…å¯†ç™»é™†node1 node2 node3

  ```shell
  ssh-copy-id master
  ssh-copy-id slave1
  ssh-copy-id slave2
  ```

-----



## äºŒã€å®‰è£…é…ç½®JDK

#### 1.åˆ›å»ºç¼–è¯‘ç¯å¢ƒè½¯ä»¶æ‰€éœ€çš„æ ¹ç›®å½•

```shell
mkdir -p /export/server
```

#### 2.å®‰è£…JDK1.8 ï¼ˆé¦–å…ˆå°† jdk-8u241-linux-x64.tar.gzå‹ç¼©åŒ…ä¸Šä¼ åˆ°/export/serverç›®å½•ï¼Œå¹¶è§£å‹ï¼‰

```shell
tar -zxvf jdk-8u241-linux-x64.tar.gz
```

#### 3.é…ç½®å…¨å±€ç¯å¢ƒå˜é‡

```shell
vim /etc/profile

export JAVA_HOME=/export/server/jdk1.8.0_241
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

#### 4.åŠ è½½ç¯å¢ƒå˜é‡

```shell
source /etc/profile
```

#### 5.æŸ¥çœ‹javaç‰ˆæœ¬

```shell
java -version
(base) [root@node1 ~]# java -version
java version "1.8.0_241"
Java(TM) SE Runtime Environment (build 1.8.0_241-b07)
Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)
```

å‡ºç°ä»¥ä¸‹å†…å®¹åˆ™ä»£è¡¨å®‰è£…æˆåŠŸ

#### 6.å°†javaæ–‡ä»¶ä»node1ä¸­ä¼ è¾“åˆ°node2 node3ä¸­ï¼Œä¼ è¾“å®Œæˆåï¼Œé…ç½®æ–¹æ³•å‚è€ƒ 3. 4.

```shell
scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/
scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/
```

#### 7. åœ¨ä¸‰å°ä¸»æœºåˆ†åˆ«åˆ›å»ºè½¯è¿æ¥ï¼ˆä¸ºäº†æ–¹ä¾¿ï¼‰

```shell
cd /export/server
ln -s jdk1.8.0_241/jdk
```

----



## ä¸‰ã€å®‰è£…é…ç½®Hadoop

#### 1.å®‰è£…Hadoop ï¼ˆé¦–å…ˆå°†hadoop-3.3.0-Centos7-with-wnappy.tar.gzå‹ç¼©åŒ…ä¸Šä¼ åˆ°/export/serverç›®å½•ï¼Œå¹¶è§£å‹ï¼‰

```shell
tar -zxvf hadoop-3.3.0-Centos7-with-wnappy.tar.gz
```

#### 2.åˆ›å»ºè½¯è¿æ¥

```shell
ln -s jhadoop-3.3.0/ hadoop
```

#### 3.ä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
cd  /export/server/hadoop/etc/hadoop
```

* ä¿®æ”¹Hadoop-env.shæ–‡ä»¶ 

  ```shell
  vim hadoop-env.sh
  
  #æœ«å°¾æ·»åŠ 
  export JAVA_HOME=/export/server/jdk
  
  export HDFS_NAMENODE_USER=root
  export HDFS_DATANODE_USER=root
  export HDFS_SECONDARYNAMENODE_USER=root
  export YARN_RESOURCEMANAGER_USER=root
  export YARN_NODEMANAGER_USER=root
  ```

* ä¿®æ”¹core-site.xmlæ–‡ä»¶

  * è®¾ç½®é»˜è®¤ç³»ç»Ÿ

    ```shell
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://node1:8020</value>
        </property>
    ```

  * è®¾ç½®hadoopæœ¬åœ°ä¿å­˜æ•°æ®è·¯å¾„

    ```
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/export/data/hadoop-3.3.0</value>
        </property>
    ```

  * è®¾ç½®HDFS web UIç”¨æˆ·èº«ä»½

    ```
        <property>
            <name>hadoop.http.staticuser.user</name>
            <value>root</value>
        </property>
    ```

  * æ•´åˆhiveç”¨æˆ·ä»£ç†è®¾ç½®

    ```
        <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
        </property>
    
        <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
        </property>
    ```

  * è®¾ç½®æ–‡ä»¶ç³»ç»Ÿåƒåœ¾æ¡¶ä¿å­˜æ—¶é—´

    ```
        <property>
            <name>fs.trash.interval</name>
            <value>1440</value>
        </property>
    ```

* é…ç½®mapred-site.xml

  * è®¾ç½®MRç¨‹åºé»˜è®¤è¿è¡Œæ¨¡å¼ï¼šyarné›†ç¾¤æ¨¡å¼ localæœ¬åœ°æ¨¡å¼

    ```
        <property>
          <name>mapreduce.framework.name</name>
          <value>yarn</value>
        </property>
    ```

  * è®¾ç½®MRç¨‹åºå†å²æœåŠ¡åœ°å€

    ```
        <property>
          <name>mapreduce.jobhistory.address</name>
          <value>node1:10020</value>
        </property>
    ```

  * è®¾ç½®MRç¨‹åºå†å²æœåŠ¡å™¨webç«¯åœ°å€

    ```
        <property>
          <name>mapreduce.jobhistory.webapp.address</name>
          <value>node1:19888</value>
        </property>
    
        <property>
          <name>yarn.app.mapreduce.am.env</name>
          <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
        </property>
    
        <property>
          <name>mapreduce.map.env</name>
          <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
        </property>
    
        <property>
          <name>mapreduce.reduce.env</name>
          <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
        </property>
    ```

* ä¿®æ”¹yarn-site.xml

  * è®¾ç½®YARNé›†ç¾¤ä¸»è§’è‰²è¿è¡Œæœºå™¨ä½ç½®

    ```
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node1</value>
        </property>
    
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    ```

  * æ˜¯å¦å°†å¯¹å®¹å™¨å®æ–½ç‰©ç†å†…å­˜é™åˆ¶

    ```
        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
          <name>yarn.log-aggregation-enable</name>
          <value>true</value>
        </property>
    ```

  * è®¾ç½®yarnå†å²æœåŠ¡å™¨åœ°å€

    ```
        <property>
            <name>yarn.log.server.url</name>
            <value>http://node1:19888/jobhistory/logs</value>
        </property>
    ```

  * å†å²æ—¥å¿—ä¿å­˜çš„æ—¶é—´ 7å¤©

    ```
        <property>
          <name>yarn.log-aggregation.retain-seconds</name>
          <value>604800</value>
        </property>
    ```

#### 4. å°†Hadoopæ–‡ä»¶ä»node1ä¸­ä¼ è¾“åˆ°node2 node3ä¸­

```shell
scp -r /export/server/hadoop/ root@node2:/export/server/
scp -r /export/server/hadoop/ root@node3:/export/server/
```

#### 5. ä¸‰å°ä¸»æœºåˆ†åˆ«é…ç½®å…¨å±€å˜é‡

```shell
export HADOOP_HOME=/export/server/hadoop-3.3.0
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### 6.é‡æ–°åŠ è½½å…¨å±€ç¯å¢ƒå˜é‡

```shell
source /etc/profile
```

#### 7.Hadoopé›†ç¾¤å¯åŠ¨å‰éœ€è¦è¿›è¡Œæ ¼å¼åŒ–

```shell
hdfs namenode -format
```

#### 8.å¯åŠ¨hadoopé›†ç¾¤

```shell
cd /export/server/hadoop/bin

start-dfs.sh
start-yarn.sh
```

#### 9.æŸ¥çœ‹å½“å‰æ‰€æœ‰è¿›ç¨‹æœåŠ¡

```shell
(base) [root@node1 ~]# jps
3040 ResourceManager
3713 Jps
3240 NodeManager
2603 DataNode
2335 NameNode

(base) [root@node2 ~]# jps
2322 Jps
2101 SecondaryNameNode
2199 NodeManager
1978 DataNode
You have new mail in /var/spool/mail/root

(base) [root@node3 ~]# jps
2225 Jps
1972 DataNode
2101 NodeManager
You have new mail in /var/spool/mail/root
```

#### 10.è¿›å…¥hdfs yarn web UIé¡µé¢

![2.png](./SparkåŸºç¡€ç¯å¢ƒé…ç½®/2.png)

![3.png](./SparkåŸºç¡€ç¯å¢ƒé…ç½®/3.png)

-----



## å››ã€å®‰è£…é…ç½®zookeeper

#### 1.å®‰è£…zookeeper ï¼ˆé¦–å…ˆå°†zookeeper-3.4.10.tar.gzå‹ç¼©åŒ…ä¸Šä¼ åˆ°/export/serverç›®å½•ï¼Œå¹¶è§£å‹ï¼‰

```shell
tar -zxvf zookeeper-3.4.10.tar.gz
```

#### 2.åˆ›å»ºè½¯è¿æ¥

```shell
ln -s zookeeper-3.4.10/ zookeeper
```

#### 3.ä¿®æ”¹é…ç½®æ–‡ä»¶

```shell
cd /export/server/zookeeper/conf/ 
```

* å°† zoo_sample.cfg æ–‡ä»¶å¤åˆ¶ä¸ºæ–°æ–‡ä»¶ zoo.cfg 

  ```shell
  cp zoo_sample.cfg zoo.cfg
  ```

* é…ç½®zoo.cfgæ–‡ä»¶

  ```shell
  vim zoo.cfg
  
  #Zookeeperçš„æ•°æ®å­˜æ”¾ç›®å½•
  dataDir=/export/server/zookeeper/zkdatas
  # ä¿ç•™å¤šå°‘ä¸ªå¿«ç…§
  autopurge.snapRetainCount=3
  # æ—¥å¿—å¤šå°‘å°æ—¶æ¸…ç†ä¸€æ¬¡
  autopurge.purgeInterval=1
  # é›†ç¾¤ä¸­æœåŠ¡å™¨åœ°å€
  server.1=node1:2888:3888
  server.2=node2:2888:3888
  server.3=node3:2888:3888
  ```

#### 4.è¿›å…¥ /export/server/zookeeper/zkdatas ç›®å½•åœ¨æ­¤ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶ï¼Œå°† 1 å†™å…¥è¿›å»

```shell
cd /export/server/zookeeper/zkdata

touch myid

echo '1' > myid
```

#### 5.å°†zookeeperæ–‡ä»¶ä»node1ä¸­ä¼ è¾“åˆ°node2 node3ä¸­

```shell
scp -r /export/server/zookeeper-3.4.10/ node2:$PWD
scp -r /export/server/zookeeper-3.4.10/ node3:$PWD
```

#### 6.åˆ›å»ºè½¯è¿æ¥

```shell
ln -s zookeeper-3.4.10/ zookeeper
```

#### 7.node2 node3åˆ†åˆ«è¿›å…¥ /export/server/zookeeper/zkdatas ç›®å½•åœ¨æ­¤ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶ï¼Œå°† 2 ï¼Œ3 å†™å…¥è¿›å»

```shell
cd /export/server/zookeeper/zkdatas/

[root@node2 zkdatas]# vim myid 
[root@node2 zkdatas]# more myid 
2

[root@node3 zkdatas]# vim myid 
[root@node3 zkdatas]# more myid 
3
```

#### 8.é…ç½®å…¨å±€ç¯å¢ƒå˜é‡

```shell
vim /etc/profile

# zookeeper ç¯å¢ƒå˜é‡
export ZOOKEEPER_HOME=/export/server/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

#### 9.é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡

```shell
source /etc/profile
```

#### 10.å¯åŠ¨zookeeperæœåŠ¡

* æ™®é€šå¯åŠ¨

  ```shell
  cd  /export/server/zookeeper-3.4.10/bin 
  
  zkServer.sh start
  ```

* ç¼–å†™è„šæœ¬è¿›è¡Œä¸€é”®å¯åŠ¨

  ```shell
  cd /bin
  
  vim zkall.sh
  
  #!/bin/bash
  if [ $# -eq 0 ]
  then
  echo "please input param:start stop"
  else
  if [ $1 = start  ]
  then
  for i in {1..3}
  do
  echo "${1}ing node${i}"
  ssh node${i} "source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start"
  done
  fi
  
  if [ $1 = stop ]
  then
  for i in {1..3}
  do
  echo "${1}ping node${i}"
  ssh node${i} "source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop"
  done
  fi
  
  if [ $1 = status ]
  then
  for i in {1..3}
  do
  echo "node${i} status:"
  ssh node${i} "source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status"
  done
  fi
  
  fi
  ```

  * ç»™ç¼–å†™çš„è„šæœ¬èµ‹äºˆå¯æ‰§è¡Œæƒé™

    ```
    chmod +x zkall.sh
    ```

  **æ³¨ï¼šå¯åŠ¨æ—¶ï¼Œæ— éœ€è¿›å…¥ä»»æ„æ–‡ä»¶ç›®å½•ä¸‹ï¼Œç›´æ¥æ‰§è¡Œzkall.sh start å³å¯**

#### 11.æŸ¥çœ‹è¿›ç¨‹

```shell
(base) [root@node1 ~]# jps
4957 Jps
4911 QuorumPeerMain

(base) [root@node2 ~]# jps
2736 QuorumPeerMain
2795 Jps
You have new mail in /var/spool/mail/root

(base) [root@node3 ~]# jps
2576 Jps
2533 QuorumPeerMain
You have new mail in /var/spool/mail/root
```

